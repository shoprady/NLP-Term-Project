{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2fed0e-4093-4c2a-bb1c-83f460413927",
   "metadata": {},
   "source": [
    "# üìç NLP Term Project\n",
    "‚≠ê **Team 5**: 2277018 Seoyeon Ye, 2277025 Eunsang Lee, 2277031 Ahyun \n",
    "\n",
    "- Setup\n",
    "  - set parameters\n",
    "  - load data\n",
    "- Construct KB\n",
    "- Categorization using LLM\n",
    "- Define prompt\n",
    "- Generate answer\n",
    "- Evaluate\n",
    "- Save the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b1f15-928c-44c5-a45e-113264798bef",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7a2ee-4187-45da-a382-e43947749eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU python-dotenv langchain langchain-community langchain-core langchain-text-splitters langchain_upstage oracledb python-dotenv faiss-cpu pymupdf4llm llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5e8ed-6e73-477e-a919-0f54bed6238b",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5acac-a628-469c-810f-ddc991192972",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"YOUR_KEY_HERE\"\n",
    "data_path = \"./\" # folder path containing samples.csv\n",
    "\n",
    "UPSTAGE_API_KEY = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22efea7-69f5-413b-997b-8e8d6828a92d",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e82ad82f68f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    prompts = data['prompts']\n",
    "    answers = data['answers']\n",
    "    return prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, answers = read_data(os.path.join(data_path, 'testset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc39614ef9dc74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prompts))\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418b869-fb3f-42db-980f-15cf029f33c7",
   "metadata": {},
   "source": [
    "## Construct KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0d68640ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "dirs = ['business', 'ewha', 'history', 'law', 'philosophy', 'psychology']\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\", api_key=UPSTAGE_API_KEY)\n",
    "\n",
    "# Initialize a dictionary to hold vector stores for each document\n",
    "vector_stores = {}\n",
    "\n",
    "for d in dirs:\n",
    "    vector_store = FAISS.load_local('./KB/{0}/'.format(d), embeddings, allow_dangerous_deserialization=True)\n",
    "    vector_stores[d] = vector_store\n",
    "    \n",
    "print(type(vector_stores['business']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc50ce",
   "metadata": {},
   "source": [
    "## Categorization using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatUpstage(api_key = UPSTAGE_API_KEY, temperature=0)\n",
    "\n",
    "prompt_template_option = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    # Request\n",
    "    You are an AI assistant designed to provide accurate and concise answers using the provided context. \n",
    "    Please follow these instructions carefully to ensure the best response:\n",
    "    \n",
    "    1. Analyze the provided Question and determine the most appropriate category(1~6) from the list below.\n",
    "    2. If the question is in Korean, the answer must always be (2) regardless of its content.\n",
    "    3. If the Question is **not in Korean**, choose one category from (1), (3), (4), (5), and (6) based on its content.\n",
    "    4. Provide the final answer in the following format (no alphabet, only numbers):\n",
    "       [ANSWER]: (X)  # Replace X with the category number (1~6)\n",
    "       \n",
    "    # Categories\n",
    "    (1) Business \n",
    "    (2) Ewha[Specific school regulations]\n",
    "    (3) History \n",
    "    (4) Law \n",
    "    (5) Philosophy \n",
    "    (6) Psychology\n",
    "    \n",
    "    ---\n",
    "    # Important Notes\n",
    "    - The answer must only indicate the category number; do not provide explanations or the answer to the question itself.\n",
    "    - The answer must always be (2) if the question is in Korean.\n",
    "    - If the question is in a language other than Korean, choose one of categories (1), (3), (4), (5), and (6).\n",
    "    - Answer which category the question belongs to, not the answer to the question itself. Keep this in mind.\n",
    "    - Ensure your response strictly matches the required format: `[ANSWER]: (X)`.\n",
    "    \n",
    "    ---\n",
    "    # Question\n",
    "    {question}\n",
    "    \n",
    "    ---\n",
    "    # Answer\n",
    "    [ANSWER]: \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_option = prompt_template_option | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfeca9b",
   "metadata": {},
   "source": [
    "## Define prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    # Request\n",
    "    You are an AI assistant designed to provide accurate and concise answers using the provided context. \n",
    "    Please follow these instructions carefully to ensure the best response:\n",
    "    \n",
    "    1. Analyze the context and identify relevant information related to the question.\n",
    "    2. Explain your reasoning step by step in detail.\n",
    "    3. Your final answer must be directly based on the context. Do not guess or infer information not present.\n",
    "    4. Use the following format for your final answer:\n",
    "       [ANSWER]: (A) `your answer here`\n",
    "    \n",
    "    ---\n",
    "    # Notes\n",
    "    - Always ensure your response is in English.\n",
    "    - The answer must be one specific option from the choices provided, based on your analysis of the given context.\n",
    "    \n",
    "    ---\n",
    "    # Question\n",
    "    {question}\n",
    "    \n",
    "    ---\n",
    "    # Context\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97b1afbe915e34",
   "metadata": {},
   "source": [
    "## Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8849a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract an answer from response\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_option(response):\n",
    "    \"\"\"\n",
    "    extracts the answer from the response using a regular expression.\n",
    "    expected format: \"[ANSWER]: (A) convolutional networks\"\n",
    "\n",
    "    if there are any answers formatted like the format, it returns None.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[ANSWER\\]:\\s*\\((1-6)\\)\"  # Regular expression to capture the answer letter and text\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1) # Extract the letter inside parentheses (e.g., A)\n",
    "    else:\n",
    "        return extract_again_option(response)\n",
    "\n",
    "def extract_again_option(response):\n",
    "    pattern = r\"\\b[1-6]\\b(?!.*\\b[1-6]\\b)\"\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b799829",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for prompt in prompts:\n",
    "    response_option = chain_option.invoke({\"question\": prompt})\n",
    "    opt = extract_option(response_option.content)\n",
    "    \n",
    "    print(opt)\n",
    "    \n",
    "    # business\n",
    "    if opt == '1':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['business']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # ewha    \n",
    "    elif opt == '2':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['ewha']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # history\n",
    "    elif opt == '3':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['history']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # law\n",
    "    elif opt == '4':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['law']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # philosophy    \n",
    "    elif opt == '5':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['philosophy']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # psychology\n",
    "    else:\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['psychology']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce96d-9367-4a3b-8df3-28f264b8143e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa582746-d69b-486e-977b-aa1e003ab2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract an answer from response\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"\n",
    "    extracts the answer from the response using a regular expression.\n",
    "    expected format: \"[ANSWER]: (A) convolutional networks\"\n",
    "\n",
    "    if there are any answers formatted like the format, it returns None.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[ANSWER\\]:\\s*\\((A-Z)\\)\"  # Regular expression to capture the answer letter and text\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1) # Extract the letter inside parentheses (e.g., A)\n",
    "    else:\n",
    "        return extract_again(response)\n",
    "\n",
    "def extract_again(response):\n",
    "    pattern = r\"\\b[A-Z]\\b(?!.*\\b[A-Z]\\b)\"\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7ff46-3d46-4332-a7d6-99b394f4c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for answer, response in zip(answers, responses):\n",
    "    print(\"-\"*10)\n",
    "    generated_answer = extract_answer(response)\n",
    "    print(response)\n",
    "    # check\n",
    "    if generated_answer:\n",
    "        print(f\"generated answer: {generated_answer}, answer: {answer}\")\n",
    "    else:\n",
    "        print(\"extraction fail\")\n",
    "\n",
    "\n",
    "    if generated_answer == None:\n",
    "        continue\n",
    "    if generated_answer in answer:\n",
    "        cnt += 1\n",
    "\n",
    "print()\n",
    "print(f\"acc: {(cnt/len(responses))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1a08b-5f7a-4a2f-ae2d-6332dcf4add9",
   "metadata": {},
   "source": [
    "## Save the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55740c8a-1699-4ad1-bd86-8633287a3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can save the answers if you need\n",
    "\"\"\"\n",
    "file_path = \"answers_{0}.txt\".format(\"history\")\n",
    "with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "    for item in responses:\n",
    "        file.write(item + \"\\n========\\n\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
