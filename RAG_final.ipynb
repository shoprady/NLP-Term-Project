{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìç NLP Term Project\n",
    "‚≠ê **Team 5**: 2277018 Seoyeon Ye, 2277025 Eunsang Lee, 2277031 Ahyun \n",
    "\n",
    "- Setup\n",
    "  - set parameters\n",
    "  - load data\n",
    "- Construct KB\n",
    "- Categorization using LLM\n",
    "- Define prompt\n",
    "- Generate answer\n",
    "- Evaluate\n",
    "- Save the answers"
   ],
   "id": "bd2fed0e-4093-4c2a-bb1c-83f460413927"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "eb1b1f15-928c-44c5-a45e-113264798bef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:14:10.321344Z",
     "start_time": "2024-12-12T01:14:01.037641Z"
    }
   },
   "cell_type": "code",
   "source": "!pip3 install -qU python-dotenv langchain langchain-community langchain-core langchain-text-splitters langchain_upstage oracledb langid python-dotenv faiss-cpu pymupdf4llm llama_index",
   "id": "86b7a2ee-4187-45da-a382-e43947749eb1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### set parameters",
   "id": "dec5e8ed-6e73-477e-a919-0f54bed6238b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:12:30.945835Z",
     "start_time": "2024-12-12T01:12:30.937732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api_key = \"YOUR_API_HERE\"\n",
    "\n",
    "data_path = \"./\" # folder path containing samples.csv\n",
    "\n",
    "UPSTAGE_API_KEY = api_key"
   ],
   "id": "d0f5acac-a628-469c-810f-ddc991192972",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### load data",
   "id": "c22efea7-69f5-413b-997b-8e8d6828a92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:14:10.756605Z",
     "start_time": "2024-12-12T01:14:10.331351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read csv file\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    prompts = data['prompts']\n",
    "    answers = data['answers']\n",
    "    return prompts, answers\n",
    "\n",
    "prompts, answers = read_data(os.path.join(data_path, 'testset.csv'))"
   ],
   "id": "16e82ad82f68f3c3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load KB",
   "id": "5418b869-fb3f-42db-980f-15cf029f33c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:22:22.262167Z",
     "start_time": "2024-12-12T01:22:21.356401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymupdf4llm\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "dirs = ['business', 'ewha', 'history', 'law', 'philosophy', 'psychology']\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\", api_key=UPSTAGE_API_KEY)\n",
    "\n",
    "# Initialize a dictionary to hold vector stores for each document\n",
    "vector_stores = {}\n",
    "\n",
    "for d in dirs:\n",
    "    vector_store = FAISS.load_local('./KB/{0}/'.format(d), embeddings, allow_dangerous_deserialization=True)\n",
    "    vector_stores[d] = vector_store\n",
    "    \n",
    "print(type(vector_stores['business']))"
   ],
   "id": "86f0d68640ddd71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Categorization using LLM",
   "id": "7ebc50ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:26:31.369776Z",
     "start_time": "2024-12-12T01:26:31.346301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import langid\n",
    "def check_ewha(query): # classify using which language used\n",
    "    lang, _ = langid.classify(query)\n",
    "    if lang == 'ko':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "id": "aca0d52ada7f9afa",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:26:32.171060Z",
     "start_time": "2024-12-12T01:26:31.585327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatUpstage(api_key = UPSTAGE_API_KEY, temperature=0)\n",
    "\n",
    "prompt_template_option = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    # Request\n",
    "    You are an AI assistant designed to provide accurate and concise answers. \n",
    "    Please follow these instructions carefully to ensure the best response:\n",
    "    \n",
    "    1. Analyze the provided Question and determine the most appropriate category(1~5) from the list below.\n",
    "    2. Choose one category based on its content.\n",
    "    3. Provide the final answer in the following format:\n",
    "       [ANSWER]: (X)  # Replace X with the category number (1~5)\n",
    "       \n",
    "    # Categories\n",
    "    (1) Business \n",
    "    (2) History \n",
    "    (3) Law \n",
    "    (4) Philosophy \n",
    "    (5) Psychology\n",
    "    \n",
    "    ---\n",
    "    # Important Notes\n",
    "    - The answer must only indicate the category number; do not provide explanations or the answer to the question itself.\n",
    "    - Answer which category the question belongs to, not the answer to the question itself. Keep this in mind.\n",
    "    - Ensure your response strictly matches the required format: `[ANSWER]: (X)`.\n",
    "    \n",
    "    ---\n",
    "    # Question\n",
    "    {question}\n",
    "    \n",
    "    ---\n",
    "    # Answer\n",
    "    [ANSWER]: \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_option = prompt_template_option | llm"
   ],
   "id": "6f9e0b84",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define prompt",
   "id": "6cfeca9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:26:34.903301Z",
     "start_time": "2024-12-12T01:26:34.897226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    # Question\n",
    "    {question}\n",
    "    \n",
    "    # Requests\n",
    "    - You are {role} intended to provide accurate and concise answers using the provided context.\n",
    "    - Solve the question step by step.\n",
    "    - Analyze the context and identify relevant information related to the question.\n",
    "    - Finish your answer with \"[ANSWER]: (X)\" where X is the correct letter choice.\n",
    "    \n",
    "    # Context\n",
    "    {context}\n",
    "    \n",
    "    # Steps:\n",
    "    Your steps here.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm"
   ],
   "id": "2921bf28102b74e4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate answer",
   "id": "ad97b1afbe915e34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T01:27:02.480660Z",
     "start_time": "2024-12-12T01:27:02.472181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to extract an answer from response\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_option(response):\n",
    "    \"\"\"\n",
    "    extracts the answer from the response using a regular expression.\n",
    "    expected format: \"[ANSWER]: (A) convolutional networks\"\n",
    "\n",
    "    if there are any answers formatted like the format, it returns None.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[ANSWER\\]:\\s*\\((1-6)\\)\"  # Regular expression to capture the answer letter and text\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1) # Extract the number inside parentheses\n",
    "    else:\n",
    "        return extract_again_option(response)\n",
    "\n",
    "def extract_again_option(response):\n",
    "    pattern = r\"\\b[1-6]\\b(?!.*\\b[1-6]\\b)\"\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return '5'"
   ],
   "id": "9e8849a0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-12T01:29:01.535536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "responses = []\n",
    "for prompt in prompts:\n",
    "    if check_ewha(prompt): # question from ewha\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['ewha']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'Ewha University Senator', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "        continue\n",
    "        \n",
    "    response_option = chain_option.invoke({\"question\": prompt})\n",
    "    opt = extract_option(response_option.content)\n",
    "    \n",
    "    print(opt)\n",
    "    \n",
    "    # business\n",
    "    if opt == '1':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['business']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'business expert', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # history\n",
    "    elif opt == '2':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['history']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'paleoanthropologist', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # law\n",
    "    elif opt == '3':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['law']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'legal expert', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # philosophy    \n",
    "    elif opt == '4':\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['philosophy']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'philosophy expert', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    \n",
    "    # psychology\n",
    "    else:\n",
    "        # Create the retriever\n",
    "        vector_store = vector_stores['psychology']\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",        # Use \"similarity\" or \"mmr\" (Maximal Marginal Relevance)\n",
    "            search_kwargs={\"k\": 5}    # Retrieve top 5 most similar chunks\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.invoke(prompt)\n",
    "        relevant_info = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"role\": 'psychology expert', \"context\": relevant_info})\n",
    "        responses.append(response.content)\n",
    "    "
   ],
   "id": "3b799829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate",
   "id": "280ce96d-9367-4a3b-8df3-28f264b8143e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# function to extract an answer from response\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"\n",
    "    extracts the answer from the response using a regular expression.\n",
    "    expected format: \"[ANSWER]: (A) convolutional networks\"\n",
    "\n",
    "    if there are any answers formatted like the format, it returns None.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[ANSWER\\]:\\s*\\((A-Z)\\)\"  # Regular expression to capture the answer letter and text\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1) # Extract the letter inside parentheses (e.g., A)\n",
    "    else:\n",
    "        return extract_again(response)\n",
    "\n",
    "def extract_again(response):\n",
    "    pattern = r\"\\b[A-Z]\\b(?!.*\\b[A-Z]\\b)\"\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None"
   ],
   "id": "fa582746-d69b-486e-977b-aa1e003ab2d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print accuracy\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for answer, response in zip(answers, responses):\n",
    "    print(\"-\"*10)\n",
    "    generated_answer = extract_answer(response)\n",
    "    print(response)\n",
    "    # check\n",
    "    if generated_answer:\n",
    "        print(f\"generated answer: {generated_answer}, answer: {answer}\")\n",
    "    else:\n",
    "        print(\"extraction fail\")\n",
    "\n",
    "\n",
    "    if generated_answer == None:\n",
    "        continue\n",
    "    if generated_answer in answer:\n",
    "        cnt += 1\n",
    "\n",
    "print()\n",
    "print(f\"acc: {(cnt/len(responses))*100}%\")"
   ],
   "id": "ebf7ff46-3d46-4332-a7d6-99b394f4c6d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the answers",
   "id": "dcd1a08b-5f7a-4a2f-ae2d-6332dcf4add9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# you can save the answers if you need\n",
    "\"\"\"\n",
    "file_path = \"answers_{0}.txt\".format(\"history\")\n",
    "with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "    for item in responses:\n",
    "        file.write(item + \"\\n========\\n\")\n",
    "\"\"\""
   ],
   "id": "55740c8a-1699-4ad1-bd86-8633287a3cd4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
